{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "colab": {
      "name": "Audio_Authentication_Inference_EN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahathaway821/audio-visual-identity-verification/blob/master/audio_authentication/Audio_Authentication_Inference_EN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUl3RAvWpG6Q",
        "colab_type": "code",
        "outputId": "46fcb69e-07dc-44e2-f4ec-c69badda10e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "pip install ibm-cos-sdk==2.0.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ibm-cos-sdk==2.0.1 in /usr/local/lib/python3.6/dist-packages (2.0.1)\n",
            "Requirement already satisfied: ibm-cos-sdk-core==2.*,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from ibm-cos-sdk==2.0.1) (2.6.0)\n",
            "Requirement already satisfied: ibm-cos-sdk-s3transfer==2.*,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from ibm-cos-sdk==2.0.1) (2.6.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk==2.0.1) (0.9.5)\n",
            "Requirement already satisfied: requests<2.23,>=2.18 in /usr/local/lib/python3.6/dist-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk==2.0.1) (2.21.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk==2.0.1) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk==2.0.1) (0.15.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<2.23,>=2.18->ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk==2.0.1) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<2.23,>=2.18->ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk==2.0.1) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<2.23,>=2.18->ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk==2.0.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<2.23,>=2.18->ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk==2.0.1) (3.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk==2.0.1) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieL4ubmSrLg6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pickle\n",
        "import ibm_boto3\n",
        "from ibm_botocore.client import Config, ClientError\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, Input, optimizers, metrics, regularizers\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "import librosa\n",
        "\n",
        "# Constants for IBM S3 values\n",
        "COS_ENDPOINT = 'https://s3.us-south.cloud-object-storage.appdomain.cloud'\n",
        "COS_API_KEY_ID = 'LKRr_5OhOyBgvHG6WH2wm9F_2bHC2sn1vV4eaCYdgpsm'\n",
        "COS_AUTH_ENDPOINT = 'https://iam.cloud.ibm.com/identity/token'\n",
        "COS_RESOURCE_CRN = 'crn:v1:bluemix:public:cloud-object-storage:global:a/ea337a3eba2f43c6b813f319db505255:0f9730a8-f2b8-42ce-b276-f9e13877a5f0::'\n",
        "\n",
        "DATA_PATH = 'data/'\n",
        "BUCKET_NAME = 'cv-audio'\n",
        "DEV_FILE = 'dev.tsv'\n",
        "AUDIO_FILE = 'dev-clips.tgz'\n",
        "MFCC_PICKLE = 'mfcc.bin'\n",
        "CLIPS_PATH = DATA_PATH + 'dev-clips/'\n",
        "TEST_CLIPS = DATA_PATH + 'test-clips/'\n",
        "BEST_MODEL_FILE = 'model-EN-20200329-133052.tgz'\n",
        "TEST_RECORDINGS = 'test-clips.tgz'\n",
        "\n",
        "PROD_PATH = DATA_PATH + 'production/'\n",
        "REF_CLIP = PROD_PATH + 'ref_clip.wav'\n",
        "REF_CLIPS_PICKLE = PROD_PATH + 'ref_clips.pickle'\n",
        "BEST_MODEL_PATH = DATA_PATH + 'production/model/'\n",
        "SCORE_THRESHOLD = 0.5\n",
        "REF_CLIPS = None\n",
        "MODEL = None\n",
        "\n",
        "bucket_name = 'cv-audio'\n",
        "\n",
        "cos = ibm_boto3.resource(\"s3\",\n",
        "    ibm_api_key_id=COS_API_KEY_ID,\n",
        "    ibm_service_instance_id=COS_RESOURCE_CRN,\n",
        "    ibm_auth_endpoint=COS_AUTH_ENDPOINT,\n",
        "    config=Config(signature_version=\"oauth\"),\n",
        "    endpoint_url=COS_ENDPOINT\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8gQbBX7CqpG",
        "colab_type": "text"
      },
      "source": [
        "### Pull Down Necessary Files from Bucket for Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iL14ecE4tmUH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load MFCC Clips\n",
        "if not os.path.exists(PROD_PATH):\n",
        "    os.makedirs(PROD_PATH)\n",
        "    \n",
        "mfccPickleResponse = cos.Object(bucket_name, MFCC_PICKLE).get()\n",
        "mfccPickle = mfccPickleResponse['Body'].read()\n",
        "#with open(REF_CLIPS_PICKLE, 'wb') as f:\n",
        "#    pickle.dump(mfccPickle, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJ85181B1gWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load model\n",
        "modelResponse = cos.Object(bucket_name, BEST_MODEL_FILE).get()\n",
        "model_tar = modelResponse['Body'].read()\n",
        "if not os.path.exists(BEST_MODEL_PATH):\n",
        "    os.makedirs(BEST_MODEL_PATH)\n",
        "with open(BEST_MODEL_PATH + BEST_MODEL_FILE , 'wb') as file:\n",
        "    file.write(model_tar)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7gBI3Iz5vH4",
        "colab_type": "code",
        "outputId": "8ce33c1e-04c2-4c26-bfa4-c3a8a257a9c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!tar zxvf /content/data/production/model/model-EN-20200329-133052.tgz -C /content/data/production/model"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EN-20200329-133052/\n",
            "EN-20200329-133052/variables/\n",
            "EN-20200329-133052/variables/variables.index\n",
            "EN-20200329-133052/variables/variables.data-00000-of-00002\n",
            "EN-20200329-133052/variables/variables.data-00001-of-00002\n",
            "EN-20200329-133052/assets/\n",
            "EN-20200329-133052/saved_model.pb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYXPELxCOlPF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_siamese_model():\n",
        "  \n",
        "    # Define the tensors for the two input images\n",
        "    left_input = Input((20, 400, 1))\n",
        "    right_input = Input((20, 400, 1))\n",
        "    \n",
        "    # Convolutional Neural Network\n",
        "    model = models.Sequential()    \n",
        "    model.add(layers.Conv2D(\n",
        "        32, \n",
        "        (10,10), \n",
        "        padding = 'same',\n",
        "        activation='relu', \n",
        "        input_shape=(20, 400, 1), \n",
        "        kernel_regularizer=regularizers.l2(2e-4)))\n",
        "    model.add(layers.MaxPooling2D())\n",
        "    \n",
        "    model.add(layers.Conv2D(\n",
        "        64, \n",
        "        (7,7),  \n",
        "        padding = 'same',\n",
        "        activation='relu',\n",
        "        kernel_regularizer=regularizers.l2(2e-4)))    \n",
        "    model.add(layers.MaxPooling2D())\n",
        "    \n",
        "    model.add(layers.Conv2D(\n",
        "        64, \n",
        "        (4,4), \n",
        "        padding = 'same', \n",
        "        activation='relu', \n",
        "        kernel_regularizer=regularizers.l2(2e-4)))\n",
        "    model.add(layers.MaxPooling2D())\n",
        "    \n",
        "    model.add(layers.Conv2D(\n",
        "        128, \n",
        "        (4,4),  \n",
        "        padding = 'same',\n",
        "        activation='relu', \n",
        "        kernel_regularizer=regularizers.l2(2e-4)))\n",
        "    \n",
        "    model.add(layers.Flatten())\n",
        "    \n",
        "    model.add(layers.Dense(\n",
        "        1024, \n",
        "        activation='sigmoid',\n",
        "        kernel_regularizer=regularizers.l2(1e-3)))\n",
        "    \n",
        "    # Generate the encodings (feature vectors) for the two images\n",
        "    encoded_l = model(left_input)\n",
        "    encoded_r = model(right_input)\n",
        "    \n",
        "    # Add a customized layer to compute the absolute difference between the encodings\n",
        "    L1_layer = layers.Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
        "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
        "    \n",
        "    # Add a dense layer with a sigmoid unit to generate the similarity score\n",
        "    prediction = layers.Dense(1, activation='sigmoid')(L1_distance)\n",
        "    \n",
        "    # Connect the inputs with the outputs\n",
        "    siamese_net = models.Model(inputs=[left_input, right_input],outputs=prediction)\n",
        "    \n",
        "    # return the model\n",
        "    return siamese_net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haDoLsVyOpGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrTz_tICCuxd",
        "colab_type": "text"
      },
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sno1G06nor_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_clips(clip_path, recorded_byte_array, min_size=1):    \n",
        "    \"\"\"\n",
        "    Splits a long clip in multiple smaller clips with MFCC length of 400. \n",
        "    Also discards the final part of the audio file the is not a multiple of 400.\n",
        "    \"\"\"\n",
        "    \n",
        "    #TODO: this function should also work with a recorded byte array\n",
        "    # since we don't need to save the user's voice everytime they try to open the door\n",
        "    \n",
        "    pad_length = 400\n",
        "    \n",
        "    wave, sr = librosa.load(clip_path, mono=True)\n",
        "    wave = wave[::3]\n",
        "    mfcc = librosa.feature.mfcc(wave, sr=sr)\n",
        "        \n",
        "    n_clips = int(mfcc.shape[1] / pad_length)\n",
        "    \n",
        "    if (n_clips < min_size):\n",
        "        raise Exception('Audio file too short. Expected MFCC equal or greater than {}, but was: {}'.format(\n",
        "            min_size*400, mfcc.shape[1]))\n",
        "    \n",
        "    # cut the recording in smaller clips\n",
        "    ref_clips = []\n",
        "    for i in range(1, n_clips+1):\n",
        "        ref_clips.append(mfcc[:, (i-1)*pad_length:i*pad_length])\n",
        "        \n",
        "    # REMOVED: a tiny part could lead to a bad score\n",
        "    # add the last clip and fill with zeros up to 400\n",
        "    #last = mfcc[:, n_clips*pad_length:mfcc.shape[1]]\n",
        "    #if(last.shape[1] > 0): \n",
        "    #    ref_clips.append(np.pad(\n",
        "    #        last, \n",
        "    #        pad_width = ((0, 0), (0, pad_length - last.shape[1])),\n",
        "    #        mode = 'constant'))\n",
        "    \n",
        "    return ref_clips\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbS8-fELor_7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def setup_voice_system():   \n",
        "    \"\"\"\n",
        "    Records the voice of the user, splits the voice in small clippings and saves\n",
        "    the clippings for later inference.\n",
        "    \"\"\"\n",
        "    \n",
        "    #TODO\n",
        "    #record the voice and save the file in path REF_CLIP\n",
        "    #if time allows, multiple users to open the door\n",
        "    \n",
        "    # expect at least 3 clips from reference recording \n",
        "    ref_clip_path = \"/content/x_english_ref.m4a\"\n",
        "    ref_clips = get_clips(ref_clip_path, 3)\n",
        "\n",
        "    return ref_clips\n",
        "    \n",
        "    with open(REF_CLIPS_PICKLE, 'wb') as handle: \n",
        "        pickle.dump(ref_clips, handle)   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYzv9HlLor_9",
        "colab_type": "code",
        "outputId": "5db8c206-5f02-48ad-b9cf-92956ee899c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def start_voice_system():\n",
        "    \"\"\"\n",
        "    Loads the reference clippings and the weights of the model.\n",
        "    \"\"\"\n",
        "    \n",
        "    #try:\n",
        "    #    with open(REF_CLIPS_PICKLE, 'rb') as handle:\n",
        "    #        ref_clips = pickle.load(handle) \n",
        "    #except Exception as e:\n",
        "    #    raise Exception('Could not find reference voice clips. Was the system set up?')\n",
        "\n",
        "    # Using pickle for ref path wasn't working? I think we can just pull from the audio file\n",
        "    ref_clip_path = \"/content/x_english_ref.m4a\"\n",
        "    ref_clips = get_clips(ref_clip_path, 3)\n",
        "\n",
        "    model = get_siamese_model()\n",
        "    model.load_weights('/content/data/production/model/EN-20200329-133052/variables/variables')\n",
        "\n",
        "    #MODEL = models.load_model(BEST_MODEL_PATH + \"EN-20200329-133052\")    \n",
        "        \n",
        "    return ref_clips, model\n",
        "BEST_MODEL_PATH + \"EN-20200329-133052\""
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'data/production/model/EN-20200329-133052'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjMs7xwtor_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_user_voice(path):\n",
        "    \"\"\"\n",
        "    Record the user voice for an attempted unlock and return smaller processed MFCC clippings\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    #recorded_byte_array = np.array()\n",
        "    \n",
        "    return get_clips(path, None, min_size=1)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezoPzF52osAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_inference_dataset(ref_clips, actual_clips):\n",
        "    left = []\n",
        "    right = []\n",
        "    \n",
        "    for ref in ref_clips:\n",
        "        for actual in actual_clips:\n",
        "            left.append([ref]) \n",
        "            right.append([actual]) \n",
        "    \n",
        "    left = np.array(left).astype(np.float32)\n",
        "    right = np.array(right).astype(np.float32)\n",
        "\n",
        "    left = np.rollaxis(np.rollaxis(left, 3, 1), 3, 1)\n",
        "    right = np.rollaxis(np.rollaxis(right, 3, 1), 3, 1)\n",
        "\n",
        "    return left, right"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-3ow7-IosAB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def speech_unlock(model, left, right, SCORE_THRESHOLD):\n",
        "    return np.mean(model.predict([left, right])) > SCORE_THRESHOLD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf3pXLCKosAC",
        "colab_type": "code",
        "outputId": "2356e2e4-b42a-47cd-8002-2063a231eabc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "\n",
        "setup_voice_system()\n",
        "\n",
        "#current error from running model on 3.5 then loading on 3.6?\n",
        "ref_clips, model = start_voice_system()\n",
        "\n",
        "#get new trial voice for attempted unlock\n",
        "actual_clips = get_user_voice(\"/content/x_english_7.m4a\")\n",
        "\n",
        "left, right = get_inference_dataset(ref_clips, actual_clips)\n",
        "\n",
        "unlock = speech_unlock(model, left, right, SCORE_THRESHOLD)\n",
        "\n",
        "#if (unlock):\n",
        "    #do something"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
            "\n",
            "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fb1100a6e48> and <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7fb11985a390>).\n",
            "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
            "\n",
            "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fb11984e908> and <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7fb11984ea90>).\n",
            "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
            "\n",
            "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fb11997fa90> and <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7fb11997fba8>).\n",
            "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
            "\n",
            "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fb1198ad6d8> and <tensorflow.python.keras.layers.core.Flatten object at 0x7fb11984e470>).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJNwfG9-osAE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ea496999-b777-4243-dc1a-51e3200d9f80"
      },
      "source": [
        "actual_clips = get_user_voice(\"/content/not_x_english_ref.m4a\")\n",
        "\n",
        "left, right = get_inference_dataset(ref_clips, actual_clips)\n",
        "\n",
        "unlock = speech_unlock(model, left, right, SCORE_THRESHOLD)\n",
        "\n",
        "print(unlock)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}