{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "colab": {
      "name": "Audio_Authentication_Inference_EN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahathaway821/audio-visual-identity-verification/blob/master/audio_authentication/Audio_Authentication_Inference_EN_tf1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUl3RAvWpG6Q",
        "colab_type": "code",
        "outputId": "1eaa16e9-3253-4f8a-c0ef-cb390c738db2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "source": [
        "pip install ibm-cos-sdk==2.0.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ibm-cos-sdk==2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/b6/01d723b26bff1c92bbce674531f93d14dbbddbf7570dd58e16dae69589ec/ibm-cos-sdk-2.0.1.tar.gz (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.0MB/s \n",
            "\u001b[?25hCollecting ibm-cos-sdk-core==2.*,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/c1/c823507c472bf88dbd045445df6850744111d34fd218c6ea3b9c9bde2cfe/ibm-cos-sdk-core-2.6.0.tar.gz (763kB)\n",
            "\u001b[K     |████████████████████████████████| 768kB 16.2MB/s \n",
            "\u001b[?25hCollecting ibm-cos-sdk-s3transfer==2.*,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/92/682a28b99777a3fdc65e6d5641ed7e1ca470d0eab3bb2826cc30c6b60e21/ibm-cos-sdk-s3transfer-2.6.0.tar.gz (221kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 48.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk==2.0.1) (0.9.5)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk==2.0.1) (0.15.2)\n",
            "Requirement already satisfied: requests<2.23,>=2.18 in /usr/local/lib/python3.6/dist-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk==2.0.1) (2.21.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk==2.0.1) (2.8.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<2.23,>=2.18->ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk==2.0.1) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<2.23,>=2.18->ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk==2.0.1) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<2.23,>=2.18->ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk==2.0.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<2.23,>=2.18->ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk==2.0.1) (2019.11.28)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk==2.0.1) (1.12.0)\n",
            "Building wheels for collected packages: ibm-cos-sdk, ibm-cos-sdk-core, ibm-cos-sdk-s3transfer\n",
            "  Building wheel for ibm-cos-sdk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-cos-sdk: filename=ibm_cos_sdk-2.0.1-py2.py3-none-any.whl size=67696 sha256=0926a00ff64d4c3fd50c303f1a90583473a7fa793e3d36ee14c60ac8529d0886\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/8f/d3/27a98712fa7eaf01c5a76b446ca53f249346dea7c686de0e4e\n",
            "  Building wheel for ibm-cos-sdk-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-cos-sdk-core: filename=ibm_cos_sdk_core-2.6.0-py2.py3-none-any.whl size=446393 sha256=7762e8d070d1c5923ee0eb2f6150637a78e2e3a9ea6e57ee0849f3d6b928223c\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/93/e6/23071b2c037147a0993d34b64a03e51abca84435fc9cd6a278\n",
            "  Building wheel for ibm-cos-sdk-s3transfer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-cos-sdk-s3transfer: filename=ibm_cos_sdk_s3transfer-2.6.0-py2.py3-none-any.whl size=89244 sha256=5b89e57de2ff7f01e838f1cc7c4c428da8f417482355d3e318a7bc617aac0d4d\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/d9/d7/43fd95b014eed89466154d8373bf4cffbb3d972de7841e213c\n",
            "Successfully built ibm-cos-sdk ibm-cos-sdk-core ibm-cos-sdk-s3transfer\n",
            "Installing collected packages: ibm-cos-sdk-core, ibm-cos-sdk-s3transfer, ibm-cos-sdk\n",
            "Successfully installed ibm-cos-sdk-2.0.1 ibm-cos-sdk-core-2.6.0 ibm-cos-sdk-s3transfer-2.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieL4ubmSrLg6",
        "colab_type": "code",
        "outputId": "3733006a-99c2-498c-d5bd-35f3a8b94424",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pickle\n",
        "import ibm_boto3\n",
        "from ibm_botocore.client import Config, ClientError\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers, models, Input, optimizers, metrics, regularizers\n",
        "from keras import backend as K\n",
        "print(tf.__version__)\n",
        "import librosa\n",
        "\n",
        "# Constants for IBM S3 values\n",
        "COS_ENDPOINT = ''\n",
        "COS_API_KEY_ID = ''\n",
        "COS_AUTH_ENDPOINT = ''\n",
        "COS_RESOURCE_CRN = ''\n",
        "\n",
        "DATA_PATH = 'data/'\n",
        "BUCKET_NAME = 'cv-audio'\n",
        "DEV_FILE = 'dev.tsv'\n",
        "AUDIO_FILE = 'dev-clips.tgz'\n",
        "MFCC_PICKLE = 'mfcc.bin'\n",
        "CLIPS_PATH = DATA_PATH + 'dev-clips/'\n",
        "TEST_CLIPS = DATA_PATH + 'test-clips/'\n",
        "BEST_MODEL_FILE = 'EN-TF1-20200405-024254.tgz'\n",
        "TEST_RECORDINGS = 'test-clips.tgz'\n",
        "\n",
        "PROD_PATH = DATA_PATH + 'production/'\n",
        "REF_CLIP = PROD_PATH + 'ref_clip.wav'\n",
        "REF_CLIPS_PICKLE = PROD_PATH + 'ref_clips.pickle'\n",
        "BEST_MODEL_PATH = DATA_PATH + 'production/model/'\n",
        "SCORE_THRESHOLD = 0.5\n",
        "REF_CLIPS = None\n",
        "MODEL = None\n",
        "\n",
        "bucket_name = 'cv-audio'\n",
        "\n",
        "cos = ibm_boto3.resource(\"s3\",\n",
        "    ibm_api_key_id=COS_API_KEY_ID,\n",
        "    ibm_service_instance_id=COS_RESOURCE_CRN,\n",
        "    ibm_auth_endpoint=COS_AUTH_ENDPOINT,\n",
        "    config=Config(signature_version=\"oauth\"),\n",
        "    endpoint_url=COS_ENDPOINT\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8gQbBX7CqpG",
        "colab_type": "text"
      },
      "source": [
        "### Pull Down Necessary Files from Bucket for Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iL14ecE4tmUH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load MFCC Clips\n",
        "if not os.path.exists(PROD_PATH):\n",
        "    os.makedirs(PROD_PATH)\n",
        "    \n",
        "#mfccPickleResponse = cos.Object(bucket_name, MFCC_PICKLE).get()\n",
        "#mfccPickle = mfccPickleResponse['Body'].read()\n",
        "#with open(REF_CLIPS_PICKLE, 'wb') as f:\n",
        "#    pickle.dump(mfccPickle, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJ85181B1gWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load model\n",
        "modelResponse = cos.Object(bucket_name, BEST_MODEL_FILE).get()\n",
        "model_tar = modelResponse['Body'].read()\n",
        "if not os.path.exists(BEST_MODEL_PATH):\n",
        "    os.makedirs(BEST_MODEL_PATH)\n",
        "with open(BEST_MODEL_PATH + BEST_MODEL_FILE , 'wb') as file:\n",
        "    file.write(model_tar)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7gBI3Iz5vH4",
        "colab_type": "code",
        "outputId": "0d314b4e-5d5d-487b-ab0a-185732a7706d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!tar zxvf /content/data/production/model/EN-TF1-20200405-024254.tgz -C /content/data/production/model"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EN-TF1-20200405-024254\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYXPELxCOlPF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_siamese_model():\n",
        "  \n",
        "    # Define the tensors for the two input images\n",
        "    left_input = Input((20, 400, 1))\n",
        "    right_input = Input((20, 400, 1))\n",
        "    \n",
        "    # Convolutional Neural Network\n",
        "    model = models.Sequential()    \n",
        "    model.add(layers.Conv2D(\n",
        "        32, \n",
        "        (10,10), \n",
        "        padding = 'same',\n",
        "        activation='relu', \n",
        "        input_shape=(20, 400, 1), \n",
        "        kernel_regularizer=regularizers.l2(2e-4)))\n",
        "    model.add(layers.MaxPooling2D())\n",
        "    \n",
        "    model.add(layers.Conv2D(\n",
        "        64, \n",
        "        (7,7),  \n",
        "        padding = 'same',\n",
        "        activation='relu',\n",
        "        kernel_regularizer=regularizers.l2(2e-4)))    \n",
        "    model.add(layers.MaxPooling2D())\n",
        "    \n",
        "    model.add(layers.Conv2D(\n",
        "        64, \n",
        "        (4,4), \n",
        "        padding = 'same', \n",
        "        activation='relu', \n",
        "        kernel_regularizer=regularizers.l2(2e-4)))\n",
        "    model.add(layers.MaxPooling2D())\n",
        "    \n",
        "    model.add(layers.Conv2D(\n",
        "        128, \n",
        "        (4,4),  \n",
        "        padding = 'same',\n",
        "        activation='relu', \n",
        "        kernel_regularizer=regularizers.l2(2e-4)))\n",
        "    \n",
        "    model.add(layers.Flatten())\n",
        "    \n",
        "    model.add(layers.Dense(\n",
        "        1024, \n",
        "        activation='sigmoid',\n",
        "        kernel_regularizer=regularizers.l2(1e-3)))\n",
        "    \n",
        "    # Generate the encodings (feature vectors) for the two images\n",
        "    encoded_l = model(left_input)\n",
        "    encoded_r = model(right_input)\n",
        "    \n",
        "    # Add a customized layer to compute the absolute difference between the encodings\n",
        "    L1_layer = layers.Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
        "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
        "    \n",
        "    # Add a dense layer with a sigmoid unit to generate the similarity score\n",
        "    prediction = layers.Dense(1, activation='sigmoid')(L1_distance)\n",
        "    \n",
        "    # Connect the inputs with the outputs\n",
        "    siamese_net = models.Model(inputs=[left_input, right_input],outputs=prediction)\n",
        "    \n",
        "    # return the model\n",
        "    return siamese_net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haDoLsVyOpGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrTz_tICCuxd",
        "colab_type": "text"
      },
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sno1G06nor_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_clips(clip_path, recorded_byte_array, min_size=1):    \n",
        "    \"\"\"\n",
        "    Splits a long clip in multiple smaller clips with MFCC length of 400. \n",
        "    Also discards the final part of the audio file the is not a multiple of 400.\n",
        "    \"\"\"\n",
        "    \n",
        "    #TODO: this function should also work with a recorded byte array\n",
        "    # since we don't need to save the user's voice everytime they try to open the door\n",
        "    \n",
        "    pad_length = 400\n",
        "    \n",
        "    wave, sr = librosa.load(clip_path, mono=True)\n",
        "    wave = wave[::3]\n",
        "    mfcc = librosa.feature.mfcc(wave, sr=sr)\n",
        "        \n",
        "    n_clips = int(mfcc.shape[1] / pad_length)\n",
        "    \n",
        "    if (n_clips < min_size):\n",
        "        raise Exception('Audio file too short. Expected MFCC equal or greater than {}, but was: {}'.format(\n",
        "            min_size*400, mfcc.shape[1]))\n",
        "    \n",
        "    # cut the recording in smaller clips\n",
        "    ref_clips = []\n",
        "    for i in range(1, n_clips+1):\n",
        "        ref_clips.append(mfcc[:, (i-1)*pad_length:i*pad_length])\n",
        "        \n",
        "    # REMOVED: a tiny part could lead to a bad score\n",
        "    # add the last clip and fill with zeros up to 400\n",
        "    #last = mfcc[:, n_clips*pad_length:mfcc.shape[1]]\n",
        "    #if(last.shape[1] > 0): \n",
        "    #    ref_clips.append(np.pad(\n",
        "    #        last, \n",
        "    #        pad_width = ((0, 0), (0, pad_length - last.shape[1])),\n",
        "    #        mode = 'constant'))\n",
        "    \n",
        "    return ref_clips\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbS8-fELor_7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def setup_voice_system():   \n",
        "    \"\"\"\n",
        "    Records the voice of the user, splits the voice in small clippings and saves\n",
        "    the clippings for later inference.\n",
        "    \"\"\"\n",
        "    \n",
        "    #TODO\n",
        "    #record the voice and save the file in path REF_CLIP\n",
        "    #if time allows, multiple users to open the door\n",
        "    \n",
        "    # expect at least 3 clips from reference recording \n",
        "    ref_clip_path = \"/content/x_english_ref.m4a\"\n",
        "    ref_clips = get_clips(ref_clip_path, 3)\n",
        "\n",
        "    return ref_clips\n",
        "    \n",
        "    with open(REF_CLIPS_PICKLE, 'wb') as handle: \n",
        "        pickle.dump(ref_clips, handle)   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYzv9HlLor_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def start_voice_system():\n",
        "    \"\"\"\n",
        "    Loads the reference clippings and the weights of the model.\n",
        "    \"\"\"\n",
        "    \n",
        "    #try:\n",
        "    #    with open(REF_CLIPS_PICKLE, 'rb') as handle:\n",
        "    #        ref_clips = pickle.load(handle) \n",
        "    #except Exception as e:\n",
        "    #    raise Exception('Could not find reference voice clips. Was the system set up?')\n",
        "\n",
        "    # Using pickle for ref path wasn't working? I think we can just pull from the audio file\n",
        "    ref_clip_path = \"/content/x_english_ref.m4a\"\n",
        "    ref_clips = get_clips(ref_clip_path, 3)\n",
        "\n",
        "    model = get_siamese_model()\n",
        "    model.load_weights('/content/data/production/model/EN-TF1-20200405-024254')\n",
        "\n",
        "    #MODEL = models.load_model(BEST_MODEL_PATH + \"EN-TF1-20200405-024254\")    \n",
        "        \n",
        "    return ref_clips, model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjMs7xwtor_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_user_voice(path):\n",
        "    \"\"\"\n",
        "    Record the user voice for an attempted unlock and return smaller processed MFCC clippings\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    #recorded_byte_array = np.array()\n",
        "    \n",
        "    return get_clips(path, None, min_size=1)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezoPzF52osAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_inference_dataset(ref_clips, actual_clips):\n",
        "    left = []\n",
        "    right = []\n",
        "    \n",
        "    for ref in ref_clips:\n",
        "        for actual in actual_clips:\n",
        "            left.append([ref]) \n",
        "            right.append([actual]) \n",
        "    \n",
        "    left = np.array(left).astype(np.float32)\n",
        "    right = np.array(right).astype(np.float32)\n",
        "\n",
        "    left = np.rollaxis(np.rollaxis(left, 3, 1), 3, 1)\n",
        "    right = np.rollaxis(np.rollaxis(right, 3, 1), 3, 1)\n",
        "\n",
        "    return left, right"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-3ow7-IosAB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def speech_unlock(model, left, right, SCORE_THRESHOLD):\n",
        "    return np.mean(model.predict([left, right])) > SCORE_THRESHOLD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf3pXLCKosAC",
        "colab_type": "code",
        "outputId": "66c9faf4-05c8-4b83-b44b-b1b2956cc7c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "setup_voice_system()\n",
        "\n",
        "#current error from running model on 3.5 then loading on 3.6?\n",
        "ref_clips, model = start_voice_system()\n",
        "\n",
        "#get new trial voice for attempted unlock\n",
        "actual_clips = get_user_voice(\"/content/x_english_7.m4a\")\n",
        "\n",
        "left, right = get_inference_dataset(ref_clips, actual_clips)\n",
        "\n",
        "unlock = speech_unlock(model, left, right, SCORE_THRESHOLD)\n",
        "print(unlock)\n",
        "#if (unlock):\n",
        "    #do something"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJNwfG9-osAE",
        "colab_type": "code",
        "outputId": "235e74ef-1342-44b5-ac5e-63b6e93c6c83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "actual_clips = get_user_voice(\"/content/not_x_english_ref.m4a\")\n",
        "\n",
        "left, right = get_inference_dataset(ref_clips, actual_clips)\n",
        "\n",
        "unlock = speech_unlock(model, left, right, SCORE_THRESHOLD)\n",
        "\n",
        "print(unlock)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}