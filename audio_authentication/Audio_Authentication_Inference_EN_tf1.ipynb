{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "colab": {
      "name": "Audio_Authentication_Inference_EN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahathaway821/audio-visual-identity-verification/blob/master/audio_authentication/Audio_Authentication_Inference_EN_tf1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUl3RAvWpG6Q",
        "colab_type": "code",
        "outputId": "274c88b1-1b22-472c-d704-59402d5c6fdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "source": [
        "pip install ibm-cos-sdk==2.0.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ibm-cos-sdk==2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/b6/01d723b26bff1c92bbce674531f93d14dbbddbf7570dd58e16dae69589ec/ibm-cos-sdk-2.0.1.tar.gz (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.1MB/s \n",
            "\u001b[?25hCollecting ibm-cos-sdk-core==2.*,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/c1/c823507c472bf88dbd045445df6850744111d34fd218c6ea3b9c9bde2cfe/ibm-cos-sdk-core-2.6.0.tar.gz (763kB)\n",
            "\u001b[K     |████████████████████████████████| 768kB 26.1MB/s \n",
            "\u001b[?25hCollecting ibm-cos-sdk-s3transfer==2.*,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/92/682a28b99777a3fdc65e6d5641ed7e1ca470d0eab3bb2826cc30c6b60e21/ibm-cos-sdk-s3transfer-2.6.0.tar.gz (221kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 54.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk==2.0.1) (0.9.5)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk==2.0.1) (0.15.2)\n",
            "Requirement already satisfied: requests<2.23,>=2.18 in /usr/local/lib/python3.6/dist-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk==2.0.1) (2.21.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk==2.0.1) (2.8.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<2.23,>=2.18->ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk==2.0.1) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<2.23,>=2.18->ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk==2.0.1) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<2.23,>=2.18->ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk==2.0.1) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<2.23,>=2.18->ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk==2.0.1) (1.24.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk==2.0.1) (1.12.0)\n",
            "Building wheels for collected packages: ibm-cos-sdk, ibm-cos-sdk-core, ibm-cos-sdk-s3transfer\n",
            "  Building wheel for ibm-cos-sdk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-cos-sdk: filename=ibm_cos_sdk-2.0.1-py2.py3-none-any.whl size=67696 sha256=8a5e2210a4ab27ebd287f6004bd13c1d244a2ba2b8ab8f89ee9f1e5f4fc314e2\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/8f/d3/27a98712fa7eaf01c5a76b446ca53f249346dea7c686de0e4e\n",
            "  Building wheel for ibm-cos-sdk-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-cos-sdk-core: filename=ibm_cos_sdk_core-2.6.0-py2.py3-none-any.whl size=446393 sha256=2d220fa19bb4fa465710d9b40227b464d692be47c6078b68defa55c05e73937e\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/93/e6/23071b2c037147a0993d34b64a03e51abca84435fc9cd6a278\n",
            "  Building wheel for ibm-cos-sdk-s3transfer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-cos-sdk-s3transfer: filename=ibm_cos_sdk_s3transfer-2.6.0-py2.py3-none-any.whl size=89244 sha256=3259ebc60776e96fa3b926091a364f43d8e30442e7eaee20782513682f7393f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/d9/d7/43fd95b014eed89466154d8373bf4cffbb3d972de7841e213c\n",
            "Successfully built ibm-cos-sdk ibm-cos-sdk-core ibm-cos-sdk-s3transfer\n",
            "Installing collected packages: ibm-cos-sdk-core, ibm-cos-sdk-s3transfer, ibm-cos-sdk\n",
            "Successfully installed ibm-cos-sdk-2.0.1 ibm-cos-sdk-core-2.6.0 ibm-cos-sdk-s3transfer-2.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieL4ubmSrLg6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a00edf46-8b97-4e8b-a9ff-24f60fccdfdf"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pickle\n",
        "import ibm_boto3\n",
        "from ibm_botocore.client import Config, ClientError\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers, models, Input, optimizers, metrics, regularizers\n",
        "from keras import backend as K\n",
        "print(tf.__version__)\n",
        "import librosa\n",
        "\n",
        "# Constants for IBM S3 values\n",
        "COS_ENDPOINT = 'https://s3.us-south.cloud-object-storage.appdomain.cloud'\n",
        "COS_API_KEY_ID = 'LKRr_5OhOyBgvHG6WH2wm9F_2bHC2sn1vV4eaCYdgpsm'\n",
        "COS_AUTH_ENDPOINT = 'https://iam.cloud.ibm.com/identity/token'\n",
        "COS_RESOURCE_CRN = 'crn:v1:bluemix:public:cloud-object-storage:global:a/ea337a3eba2f43c6b813f319db505255:0f9730a8-f2b8-42ce-b276-f9e13877a5f0::'\n",
        "\n",
        "DATA_PATH = 'data/'\n",
        "BUCKET_NAME = 'cv-audio'\n",
        "DEV_FILE = 'dev.tsv'\n",
        "AUDIO_FILE = 'dev-clips.tgz'\n",
        "MFCC_PICKLE = 'mfcc.bin'\n",
        "CLIPS_PATH = DATA_PATH + 'dev-clips/'\n",
        "TEST_CLIPS = DATA_PATH + 'test-clips/'\n",
        "BEST_MODEL_FILE = 'EN-TF1-20200405-024254.tgz'\n",
        "TEST_RECORDINGS = 'test-clips.tgz'\n",
        "\n",
        "PROD_PATH = DATA_PATH + 'production/'\n",
        "REF_CLIP = PROD_PATH + 'ref_clip.wav'\n",
        "REF_CLIPS_PICKLE = PROD_PATH + 'ref_clips.pickle'\n",
        "BEST_MODEL_PATH = DATA_PATH + 'production/model/'\n",
        "SCORE_THRESHOLD = 0.5\n",
        "REF_CLIPS = None\n",
        "MODEL = None\n",
        "\n",
        "bucket_name = 'cv-audio'\n",
        "\n",
        "cos = ibm_boto3.resource(\"s3\",\n",
        "    ibm_api_key_id=COS_API_KEY_ID,\n",
        "    ibm_service_instance_id=COS_RESOURCE_CRN,\n",
        "    ibm_auth_endpoint=COS_AUTH_ENDPOINT,\n",
        "    config=Config(signature_version=\"oauth\"),\n",
        "    endpoint_url=COS_ENDPOINT\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8gQbBX7CqpG",
        "colab_type": "text"
      },
      "source": [
        "### Pull Down Necessary Files from Bucket for Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iL14ecE4tmUH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load MFCC Clips\n",
        "if not os.path.exists(PROD_PATH):\n",
        "    os.makedirs(PROD_PATH)\n",
        "    \n",
        "#mfccPickleResponse = cos.Object(bucket_name, MFCC_PICKLE).get()\n",
        "#mfccPickle = mfccPickleResponse['Body'].read()\n",
        "#with open(REF_CLIPS_PICKLE, 'wb') as f:\n",
        "#    pickle.dump(mfccPickle, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJ85181B1gWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load model\n",
        "modelResponse = cos.Object(bucket_name, BEST_MODEL_FILE).get()\n",
        "model_tar = modelResponse['Body'].read()\n",
        "if not os.path.exists(BEST_MODEL_PATH):\n",
        "    os.makedirs(BEST_MODEL_PATH)\n",
        "with open(BEST_MODEL_PATH + BEST_MODEL_FILE , 'wb') as file:\n",
        "    file.write(model_tar)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7gBI3Iz5vH4",
        "colab_type": "code",
        "outputId": "489a824e-491d-4631-924a-028f4a5c0eda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!tar zxvf /content/data/production/model/EN-TF1-20200405-024254.tgz -C /content/data/production/model"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EN-TF1-20200405-024254\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYXPELxCOlPF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_siamese_model():\n",
        "  \n",
        "    # Define the tensors for the two input images\n",
        "    left_input = Input((20, 400, 1))\n",
        "    right_input = Input((20, 400, 1))\n",
        "    \n",
        "    # Convolutional Neural Network\n",
        "    model = models.Sequential()    \n",
        "    model.add(layers.Conv2D(\n",
        "        32, \n",
        "        (10,10), \n",
        "        padding = 'same',\n",
        "        activation='relu', \n",
        "        input_shape=(20, 400, 1), \n",
        "        kernel_regularizer=regularizers.l2(2e-4)))\n",
        "    model.add(layers.MaxPooling2D())\n",
        "    \n",
        "    model.add(layers.Conv2D(\n",
        "        64, \n",
        "        (7,7),  \n",
        "        padding = 'same',\n",
        "        activation='relu',\n",
        "        kernel_regularizer=regularizers.l2(2e-4)))    \n",
        "    model.add(layers.MaxPooling2D())\n",
        "    \n",
        "    model.add(layers.Conv2D(\n",
        "        64, \n",
        "        (4,4), \n",
        "        padding = 'same', \n",
        "        activation='relu', \n",
        "        kernel_regularizer=regularizers.l2(2e-4)))\n",
        "    model.add(layers.MaxPooling2D())\n",
        "    \n",
        "    model.add(layers.Conv2D(\n",
        "        128, \n",
        "        (4,4),  \n",
        "        padding = 'same',\n",
        "        activation='relu', \n",
        "        kernel_regularizer=regularizers.l2(2e-4)))\n",
        "    \n",
        "    model.add(layers.Flatten())\n",
        "    \n",
        "    model.add(layers.Dense(\n",
        "        1024, \n",
        "        activation='sigmoid',\n",
        "        kernel_regularizer=regularizers.l2(1e-3)))\n",
        "    \n",
        "    # Generate the encodings (feature vectors) for the two images\n",
        "    encoded_l = model(left_input)\n",
        "    encoded_r = model(right_input)\n",
        "    \n",
        "    # Add a customized layer to compute the absolute difference between the encodings\n",
        "    L1_layer = layers.Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
        "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
        "    \n",
        "    # Add a dense layer with a sigmoid unit to generate the similarity score\n",
        "    prediction = layers.Dense(1, activation='sigmoid')(L1_distance)\n",
        "    \n",
        "    # Connect the inputs with the outputs\n",
        "    siamese_net = models.Model(inputs=[left_input, right_input],outputs=prediction)\n",
        "    \n",
        "    # return the model\n",
        "    return siamese_net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haDoLsVyOpGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrTz_tICCuxd",
        "colab_type": "text"
      },
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sno1G06nor_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_clips(clip_path, recorded_byte_array, min_size=1):    \n",
        "    \"\"\"\n",
        "    Splits a long clip in multiple smaller clips with MFCC length of 400. \n",
        "    Also discards the final part of the audio file the is not a multiple of 400.\n",
        "    \"\"\"\n",
        "    \n",
        "    #TODO: this function should also work with a recorded byte array\n",
        "    # since we don't need to save the user's voice everytime they try to open the door\n",
        "    \n",
        "    pad_length = 400\n",
        "    \n",
        "    wave, sr = librosa.load(clip_path, mono=True)\n",
        "    wave = wave[::3]\n",
        "    mfcc = librosa.feature.mfcc(wave, sr=sr)\n",
        "        \n",
        "    n_clips = int(mfcc.shape[1] / pad_length)\n",
        "    \n",
        "    if (n_clips < min_size):\n",
        "        raise Exception('Audio file too short. Expected MFCC equal or greater than {}, but was: {}'.format(\n",
        "            min_size*400, mfcc.shape[1]))\n",
        "    \n",
        "    # cut the recording in smaller clips\n",
        "    ref_clips = []\n",
        "    for i in range(1, n_clips+1):\n",
        "        ref_clips.append(mfcc[:, (i-1)*pad_length:i*pad_length])\n",
        "        \n",
        "    # REMOVED: a tiny part could lead to a bad score\n",
        "    # add the last clip and fill with zeros up to 400\n",
        "    #last = mfcc[:, n_clips*pad_length:mfcc.shape[1]]\n",
        "    #if(last.shape[1] > 0): \n",
        "    #    ref_clips.append(np.pad(\n",
        "    #        last, \n",
        "    #        pad_width = ((0, 0), (0, pad_length - last.shape[1])),\n",
        "    #        mode = 'constant'))\n",
        "    \n",
        "    return ref_clips\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbS8-fELor_7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def setup_voice_system():   \n",
        "    \"\"\"\n",
        "    Records the voice of the user, splits the voice in small clippings and saves\n",
        "    the clippings for later inference.\n",
        "    \"\"\"\n",
        "    \n",
        "    #TODO\n",
        "    #record the voice and save the file in path REF_CLIP\n",
        "    #if time allows, multiple users to open the door\n",
        "    \n",
        "    # expect at least 3 clips from reference recording \n",
        "    ref_clip_path = \"/content/x_english_ref.m4a\"\n",
        "    ref_clips = get_clips(ref_clip_path, 3)\n",
        "\n",
        "    return ref_clips\n",
        "    \n",
        "    with open(REF_CLIPS_PICKLE, 'wb') as handle: \n",
        "        pickle.dump(ref_clips, handle)   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYzv9HlLor_9",
        "colab_type": "code",
        "outputId": "62f8114b-affe-4bf3-b3b3-9c48aa12116a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def start_voice_system():\n",
        "    \"\"\"\n",
        "    Loads the reference clippings and the weights of the model.\n",
        "    \"\"\"\n",
        "    \n",
        "    #try:\n",
        "    #    with open(REF_CLIPS_PICKLE, 'rb') as handle:\n",
        "    #        ref_clips = pickle.load(handle) \n",
        "    #except Exception as e:\n",
        "    #    raise Exception('Could not find reference voice clips. Was the system set up?')\n",
        "\n",
        "    # Using pickle for ref path wasn't working? I think we can just pull from the audio file\n",
        "    ref_clip_path = \"/content/x_english_ref.m4a\"\n",
        "    ref_clips = get_clips(ref_clip_path, 3)\n",
        "\n",
        "    model = get_siamese_model()\n",
        "    model.load_weights('/content/data/production/model/EN-TF1-20200405-024254/variables/variables')\n",
        "\n",
        "    #MODEL = models.load_model(BEST_MODEL_PATH + \"EN-TF1-20200405-024254\")    \n",
        "        \n",
        "    return ref_clips, model\n",
        "BEST_MODEL_PATH + \"EN-20200329-133052\""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'data/production/model/EN-20200329-133052'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjMs7xwtor_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_user_voice(path):\n",
        "    \"\"\"\n",
        "    Record the user voice for an attempted unlock and return smaller processed MFCC clippings\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    #recorded_byte_array = np.array()\n",
        "    \n",
        "    return get_clips(path, None, min_size=1)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezoPzF52osAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_inference_dataset(ref_clips, actual_clips):\n",
        "    left = []\n",
        "    right = []\n",
        "    \n",
        "    for ref in ref_clips:\n",
        "        for actual in actual_clips:\n",
        "            left.append([ref]) \n",
        "            right.append([actual]) \n",
        "    \n",
        "    left = np.array(left).astype(np.float32)\n",
        "    right = np.array(right).astype(np.float32)\n",
        "\n",
        "    left = np.rollaxis(np.rollaxis(left, 3, 1), 3, 1)\n",
        "    right = np.rollaxis(np.rollaxis(right, 3, 1), 3, 1)\n",
        "\n",
        "    return left, right"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-3ow7-IosAB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def speech_unlock(model, left, right, SCORE_THRESHOLD):\n",
        "    return np.mean(model.predict([left, right])) > SCORE_THRESHOLD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf3pXLCKosAC",
        "colab_type": "code",
        "outputId": "ec9b8a19-3391-4603-d562-45b7a7bbc133",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        }
      },
      "source": [
        "\n",
        "setup_voice_system()\n",
        "\n",
        "#current error from running model on 3.5 then loading on 3.6?\n",
        "ref_clips, model = start_voice_system()\n",
        "\n",
        "#get new trial voice for attempted unlock\n",
        "actual_clips = get_user_voice(\"/content/x_english_7.m4a\")\n",
        "\n",
        "left, right = get_inference_dataset(ref_clips, actual_clips)\n",
        "\n",
        "unlock = speech_unlock(model, left, right, SCORE_THRESHOLD)\n",
        "\n",
        "#if (unlock):\n",
        "    #do something"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_get_default_graph\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'get_default_graph'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-dd58366da194>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#current error from running model on 3.5 then loading on 3.6?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mref_clips\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_voice_system\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#get new trial voice for attempted unlock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-88deab7bdc5e>\u001b[0m in \u001b[0;36mstart_voice_system\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mref_clips\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_clips\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref_clip_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_siamese_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/data/production/model/EN-TF1-20200405-024254/variables/variables'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-f9580d70f056>\u001b[0m in \u001b[0;36mget_siamese_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Define the tensors for the two input images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mleft_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mright_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/input_layer.py\u001b[0m in \u001b[0;36mInput\u001b[0;34m(shape, batch_shape, name, dtype, sparse, tensor)\u001b[0m\n\u001b[1;32m    176\u001b[0m                              \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                              \u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                              input_tensor=tensor)\n\u001b[0m\u001b[1;32m    179\u001b[0m     \u001b[0;31m# Return tensor including _keras_shape and _keras_history.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;31m# Note that in this case train_output and test_output are the same pointer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/input_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_shape, batch_size, batch_input_shape, dtype, input_tensor, sparse, name)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'input'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_uid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInputLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_uid\u001b[0;34m(prefix)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_get_default_graph\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         raise RuntimeError(\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;34m'It looks like you are trying to use '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;34m'a version of multi-backend Keras that '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;34m'does not support TensorFlow 2.0. We recommend '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: It looks like you are trying to use a version of multi-backend Keras that does not support TensorFlow 2.0. We recommend using `tf.keras`, or alternatively, downgrading to TensorFlow 1.14."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJNwfG9-osAE",
        "colab_type": "code",
        "outputId": "ea496999-b777-4243-dc1a-51e3200d9f80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "actual_clips = get_user_voice(\"/content/not_x_english_ref.m4a\")\n",
        "\n",
        "left, right = get_inference_dataset(ref_clips, actual_clips)\n",
        "\n",
        "unlock = speech_unlock(model, left, right, SCORE_THRESHOLD)\n",
        "\n",
        "print(unlock)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}